{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2, Problem 4\n",
    "\n",
    "The goal of this problem is for you to try and classify whether or not an individual is likely to make more or less than 50K per year.  Carry out this task.  Try at least five machine algorithms, report precision, recall and f1 score on a test set.\n",
    "\n",
    "For each of the parts, report your preformance in terms not of just numbers but in terms of graphs. When you have training and validation data, please show the curves as the training progresses. You should know when you are overfitting or underfitting. Don't just report bare numbers. **You are free to add implmentation or markdown cells to make your notebook clearer!!**\n",
    "\n",
    "## Data:\n",
    "\n",
    "The following dataset was taken from the first dataset repository: http://archive.ics.uci.edu/ml/datasets/Adult\n",
    "\n",
    "As the original task of the dataset lays out, \n",
    "Please note:\n",
    "* the continuous variable fnlwgt represents final weight, which is the number of units in the target population that the responding unit represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dealing with Missing Values\n",
    "\n",
    "What should you do about dealing with missing values - do you just drop those rows?\n",
    "\n",
    "One of the most common problems we come accross in working with data \"in the wild\" is missing data. Often we will have observations (rows) that have only some of the needed attributes. Different rows will have different attributes missing. There are a number of strategies for dealing with the missing values. Clearly one could be dropping the column (attribute), or row (observation). Unfortuntely if you drop columns you may lose critical information that is helpful for classification and may be present in most (many) of the rows. You can drop rows but if many rows have at least one missing value, you may loose too much data. Do you try to impute (i. e. fill in) the missing data?  If so how?  \n",
    "\n",
    "Explain why you chose the strategy you did.\n",
    "\n",
    "*Hint - '?' denotes a missing value.*\n",
    "\n",
    "### Some possible strategies for dealing with missing data\n",
    "\n",
    "1. Whenever there is pleanty of data and very little missing data, you should consider dropping rows and/or columns. This may introduce some bias in the data but again, if the problem is limited to a very few rows or columns, it is easy in training to reproduce.\n",
    "\n",
    "2. Fill with fixed value using sklearn.impute.SimpleImputer.\n",
    "    a. 'constant' 0. Rarely a good idea but sometimes, if we can assume that when it is missing it is basically 0, this might be a good idea. For example a data may list number of house fires in a zip code and a missing value just means none.\n",
    "    b. 'mean' if the data is numeric, the mean is meaningfull.\n",
    "    c. 'median' may be more sensible if the data is integer or ordered. When the mean and median are very different it is important to understand what a \"typical\" example might mean. When considering \"income\", for example, a few large outliers will mess up the mean.\n",
    "    d. \"most_frequent' when you have categorical (nominal) labels, mean and median don't make any sense. Most probable label is what you need to use. This is also known as \"mode\".\n",
    "\n",
    "3. sklearn.impute.MissingIndicator: Sometimes the fact that a value is missing, is itself an important indicator. One can create a new feature/attribute that indicates a certain attribute is missing. If you later build a classifier by hand you can explicitly wieght each variable using the missing variable weights so that for that example (row) that attribute won't contribute to the classifier. In a deep neural network it is possible that the network can learn to do that automatically.\n",
    "\n",
    "4. One can use the sklearn.impute.KNNImputer which will look for rows to fill in the data.\n",
    "\n",
    "5. Fill with sklearn.impute.IterativeImputer scikit-learn provides a sophisticated imputation strategy. You can read up on this in the documentation, but it will fix on of the columns (attributes), and try to use the other features to predict similar to KNN but more sophisticated.\n",
    "\n",
    "6. Train a classifier: You can build your own classifier using machine learning. This is kind of a problem within a problem but if done correctly, it has the potential to be more accurate than a simpler method. Of course, if done badly it could be worse.\n",
    "\n",
    "7. Manually impute the missing values. You may know enough about the problem to build an ad-hoc way to fill in the missing values for each column in a way that makes the most sense. This almost always requires a great deal of domain expertise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      "age               32561 non-null int64\n",
      "work_class        32561 non-null object\n",
      "fnlwgt            32561 non-null int64\n",
      "education         32561 non-null object\n",
      "education_num     32561 non-null int64\n",
      "marital_status    32561 non-null object\n",
      "occupation        32561 non-null object\n",
      "relationship      32561 non-null object\n",
      "race              32561 non-null object\n",
      "sex               32561 non-null object\n",
      "capital_gain      32561 non-null int64\n",
      "capital_loss      32561 non-null int64\n",
      "hours_per_week    32561 non-null int64\n",
      "native_country    32561 non-null object\n",
      "target            32561 non-null object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Add your code for filling in the data here. Please end by using the appropriate method for data filling.\n",
    "# to show the amount of missing data (which in the end should not be any since you dropped or filled in data)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "columns = [\n",
    "    \"age\",\n",
    "    \"work_class\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"target\"\n",
    "]\n",
    "df = pd.read_csv(\"C:/Users/Cemil Turhan/Downloads/adult.data\", names=columns)\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        df[column] = df[column].str.strip()\n",
    "#print(df.describe)\n",
    "#print(df.info)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train Test Validate Split\n",
    "\n",
    "Ideally you will split the data and use the train data filling in proceedure for the test data. Because this is expensive you can do experiments initially to see if this matters. Just keep carefully in mind what you will know and what you can't know during the test evaluation. Both sklearn and tensorflow provide facilities for train test split. Take your pick.\n",
    "\n",
    "At the end of this you should have a train, validate and test split. In the next part you are going to do preliminary testing of your model with your train+validation sets to get some idea of good canditates for hyperparameters. Later you will merge your training and validation set and resplit them up using cross validation to get better estimates for setting hyper-parameters\n",
    "\n",
    "**NOTE: It is very important that you record very carefully any parameters you have for filling in data in step 1. For example if you you build a \"fit\" using some training data, later you will need to use the this \"fit\" to transform the data, you can not re-fit on new data. In other words if your \"pipline\" in training takes the mean of the input to fill in the first column, you need to fill with exactly that number, when you get new data for testing. Don't take the mean of the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               State-gov\n",
       "1        Self-emp-not-inc\n",
       "2                 Private\n",
       "3                 Private\n",
       "4                 Private\n",
       "5                 Private\n",
       "6                 Private\n",
       "7        Self-emp-not-inc\n",
       "8                 Private\n",
       "9                 Private\n",
       "10                Private\n",
       "11              State-gov\n",
       "12                Private\n",
       "13                Private\n",
       "14                Private\n",
       "15                Private\n",
       "16       Self-emp-not-inc\n",
       "17                Private\n",
       "18                Private\n",
       "19       Self-emp-not-inc\n",
       "20                Private\n",
       "21                Private\n",
       "22            Federal-gov\n",
       "23                Private\n",
       "24                Private\n",
       "25              Local-gov\n",
       "26                Private\n",
       "27                Private\n",
       "28                Private\n",
       "29                Private\n",
       "               ...       \n",
       "32531             Private\n",
       "32532             Private\n",
       "32533             Private\n",
       "32534             Private\n",
       "32535             Private\n",
       "32536             Private\n",
       "32537             Private\n",
       "32538             Private\n",
       "32539             Private\n",
       "32540           State-gov\n",
       "32541             Private\n",
       "32542             Private\n",
       "32543           Local-gov\n",
       "32544             Private\n",
       "32545           Local-gov\n",
       "32546             Private\n",
       "32547             Private\n",
       "32548    Self-emp-not-inc\n",
       "32549           State-gov\n",
       "32550    Self-emp-not-inc\n",
       "32551             Private\n",
       "32552             Private\n",
       "32553             Private\n",
       "32554             Private\n",
       "32555             Private\n",
       "32556             Private\n",
       "32557             Private\n",
       "32558             Private\n",
       "32559             Private\n",
       "32560        Self-emp-inc\n",
       "Name: work_class, Length: 32561, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill your solution for a train-test split in here.\n",
    "\n",
    "\n",
    "#convert all numericals to float\n",
    "df.age = df.age.astype(float)\n",
    "df.fnlwgt = df.fnlwgt.astype(float)\n",
    "df.education_num = df.education_num.astype(float)\n",
    "df.hours_per_week = df.hours_per_week.astype(float)\n",
    "\n",
    "#has many zeros, not meaningful\n",
    "df.drop(\"capital_loss\", axis=1, inplace=True,)\n",
    "df.drop(\"capital_gain\", axis=1, inplace=True,)\n",
    "\n",
    "\n",
    "num_quest=df[df[\"work_class\"]==\"?\"]\n",
    "#print(num_quest)\n",
    "df.work_class.value_counts() #Private is the most seen one   \n",
    "\n",
    "df.work_class.replace(\"?\", \"Private\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "num_quest=df[df[\"fnlwgt\"]==\"?\"]  #checked fnlwgt, education, education_num, marital_status, relationship, race, sex,capital_gain, capital_loss, hours_per_week\n",
    "#print(num_quest)\n",
    "\n",
    "num_quest=df[df[\"occupation\"]==\"?\"]\n",
    "#print(num_quest)\n",
    "df.occupation.value_counts()    \n",
    "\n",
    "df=df[df[\"occupation\"]!=\"?\"]  #I drop the null values in occupation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_quest=df[df[\"native_country\"]==\"?\"]\n",
    "#print(num_quest)\n",
    "df.native_country.value_counts()\n",
    "df.native_country.replace(\"?\", \"United-States\") #United_States is the most seen one\n",
    "\n",
    "\n",
    "df[\"target\"] = df[\"target\"].map({ \"<=50K\": -1, \">50K\": 1 })\n",
    "\n",
    "#one-hot encoding for non-numerical columns\n",
    "df = pd.get_dummies(df, columns=[\n",
    "    \"work_class\", \"education\", \"marital_status\", \"occupation\", \"relationship\",\n",
    "    \"race\", \"sex\", \"native_country\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target variable\n",
    "data_x = df.iloc[:,:-1]\n",
    "data_y = df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(data_x, data_y, test_size=0.25, stratify=data_y, random_state=0) \n",
    "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train, test_size=0.25,  stratify=y_train, random_state=42) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build different five different variations sklearn models and a Dummy\n",
    "\n",
    "You will need to use a baseline classifier. Sklearn has sklearn.dummy.DummyClassifier which you can use as a benchmark for a braindead classifier. Pick 5 classifiers including simple ones like Knn or linear like logistic regression, and sophistocated ones like random forest and svm. Use the training and validation data from above (don't look at the testing data). Get a baseline for performance.\n",
    "\n",
    "Create bar graphs using different metrics including accuracy, recall, precision and f1 score accross the different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline results here with logisic regression and random forest\n",
    "\n",
    "# Set up your models here\n",
    "\n",
    "def model_one():\n",
    "    logreg=LogisticRegression()\n",
    "    fit=logreg.fit(X_tr, y_tr)\n",
    "    y_pred=logreg.predict(X_val)\n",
    "    score=logreg.score(X_val,y_val)\n",
    "    conf_mat=confusion_matrix(y_val,y_pred)\n",
    "    clas_rep=classification_report(y_val,y_pred)\n",
    "    print(score)\n",
    "    print(conf_mat)\n",
    "    print(clas_rep)\n",
    "\n",
    "def model_two():\n",
    "    knn=KNeighborsClassifier(n_neighbors=6)\n",
    "    fit=knn.fit(X_tr, y_tr)\n",
    "    y_pred=knn.predict(X_val)\n",
    "    score_2=knn.score(X_val,y_val)\n",
    "    conf_mat_2=confusion_matrix(y_val,y_pred)\n",
    "    clas_rep_2=classification_report(y_val,y_pred)\n",
    "    print(score_2)\n",
    "    print(conf_mat_2)\n",
    "    print(clas_rep_2)\n",
    "\n",
    "def model_three():\n",
    "    dt=DecisionTreeClassifier(random_state=0)\n",
    "    fit=dt.fit(X_tr, y_tr)\n",
    "    y_pred=dt.predict(X_val)\n",
    "    score_3=dt.score(X_val,y_val)\n",
    "    conf_mat_3=confusion_matrix(y_val,y_pred)\n",
    "    clas_rep_3=classification_report(y_val,y_pred)\n",
    "    print(score_3)\n",
    "    print(conf_mat_3)\n",
    "    print(clas_rep_3)\n",
    "\n",
    "def model_four():\n",
    "    svm=LinearSVC(random_state=0)\n",
    "    fit=svm.fit(X_tr, y_tr)\n",
    "    y_pred=svm.predict(X_val)\n",
    "    score_4=svm.score(X_val,y_val)\n",
    "    conf_mat_4=confusion_matrix(y_val,y_pred)\n",
    "    clas_rep_4=classification_report(y_val,y_pred)\n",
    "    print(score_4)\n",
    "    print(conf_mat_4)\n",
    "    print(clas_rep_4)\n",
    "\n",
    "def model_five():\n",
    "    rf=RandomForestClassifier(max_depth=3, random_state=0)\n",
    "    fit=rf.fit(X_tr, y_tr)\n",
    "    y_pred=rf.predict(X_val)\n",
    "    score_5=rf.score(X_val,y_val)\n",
    "    conf_mat_5=confusion_matrix(y_val,y_pred)\n",
    "    clas_rep_5=classification_report(y_val,y_pred)\n",
    "    print(score_5)\n",
    "    print(conf_mat_5)\n",
    "    print(clas_rep_5)\n",
    "    \n",
    "\n",
    "def model_baseline():\n",
    "    dummy = DummyClassifier(strategy=\"stratified\")\n",
    "    fit=dummy.fit(X_tr, y_tr)\n",
    "    y_pred=dummy.predict(X_val)\n",
    "    score_base=dummy.score(X_val,y_val)\n",
    "    conf_mat_base=confusion_matrix(y_val,y_pred)\n",
    "    clas_rep_base=classification_report(y_val,y_pred)\n",
    "    print(score_base)\n",
    "    print(conf_mat_base)\n",
    "    print(clas_rep_base)\n",
    "    \n",
    "# Perform preliminary evaluations here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994791666666667\n",
      "[[5754    3]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994791666666667\n",
      "[[5757    0]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994791666666667\n",
      "[[5757    0]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n",
      "0.9987847222222223\n",
      "[[5753    4]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n",
      "0.9994791666666667\n",
      "[[5757    0]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n",
      "0.9994791666666667\n",
      "[[5757    0]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model_baseline()\n",
    "model_one()\n",
    "model_two()\n",
    "model_three()\n",
    "model_four()\n",
    "model_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary conclusions on your models\n",
    "\n",
    "Include some graphs and peformance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cross-validation\n",
    "We really should have used k-fold (eg. k=5) crossvalidation here, to not only evaluate our five keras/tensorflow models. See how your preliminary results change. Now that we have validation results with uncertainy (+- standard deviation), do your prior conclusion change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994791666390718\n",
      "0.9994791666390718\n",
      "0.9986653327148509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994791666390718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994791666390718\n"
     ]
    }
   ],
   "source": [
    "# Part 4 inplement cross validation here\n",
    "from sklearn.model_selection import cross_val_score\n",
    "logreg=LogisticRegression()   \n",
    "scores_1=cross_val_score(logreg, data_x, data_y, cv=10)\n",
    "print(scores_1.mean())\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=6)   \n",
    "scores_2=cross_val_score(knn, data_x, data_y, cv=10)\n",
    "print(scores_2.mean())\n",
    "\n",
    "dt=DecisionTreeClassifier(random_state=0)   \n",
    "scores_3=cross_val_score(dt, data_x, data_y, cv=10)\n",
    "print(scores_3.mean())\n",
    "\n",
    "svm=LinearSVC(random_state=0)  \n",
    "scores_4=cross_val_score(svm, data_x, data_y, cv=10)\n",
    "print(scores_4.mean())\n",
    "\n",
    "rf=RandomForestClassifier(max_depth=3, random_state=0)  \n",
    "scores_5=cross_val_score(rf, data_x, data_y, cv=10)\n",
    "print(scores_5.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in your Part 4 Conclusion here \n",
    "\n",
    "After cross validation, accuracies didn't change too much because the previous accuracies were also high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Refining with Regularization\n",
    "\n",
    "We know that our biggest problem, if our models are flexibile enough, will be overfitting. Please try to regularize your best 2 models to see if you can improve their results. Not all algorithms have regularization but analyze two that do. Make sure you show graph performance has you change the regularization parameters.\n",
    "Look at these questions:\n",
    "\n",
    "* Try regularizing each of your two best models, does the generalizability increase?  of Decrease?  \n",
    "* Is one more sensitive than the other? Why might this happen and why?  \n",
    "* Please try this with all of your features and then with the reduced set of features.  \n",
    "* Report your precision, recall and f1 score on the train and validation sets (no cross validatio yet).  \n",
    "* Next carry out cross validation.  Does regularization reduce under or overfitting?   Why or why not?  \n",
    "\n",
    "** Hint: Try both L1 or L2 norm for regularization or dropout **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994791666666667\n",
      "[[5757    0]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n",
      "0.9994791666666667\n",
      "[[5757    0]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n",
      "0.9994791666666667\n",
      "[[5757    0]\n",
      " [   3    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5757\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           1.00      5760\n",
      "   macro avg       0.50      0.50      0.50      5760\n",
      "weighted avg       1.00      1.00      1.00      5760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Cemil Turhan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Fill in your code analysis for part 5 here\n",
    "logreg=LogisticRegression(penalty=\"l1\")\n",
    "fit=logreg.fit(X_tr, y_tr)\n",
    "y_pred=logreg.predict(X_val)\n",
    "score=logreg.score(X_val,y_val)\n",
    "conf_mat=confusion_matrix(y_val,y_pred)\n",
    "clas_rep=classification_report(y_val,y_pred)\n",
    "print(score)\n",
    "print(conf_mat)\n",
    "print(clas_rep)\n",
    "\n",
    "\n",
    "logreg=LogisticRegression(penalty=\"l2\")\n",
    "fit=logreg.fit(X_tr, y_tr)\n",
    "y_pred=logreg.predict(X_val)\n",
    "score=logreg.score(X_val,y_val)\n",
    "conf_mat=confusion_matrix(y_val,y_pred)\n",
    "clas_rep=classification_report(y_val,y_pred)\n",
    "print(score)\n",
    "print(conf_mat)\n",
    "print(clas_rep)\n",
    "    \n",
    "svm=LinearSVC(random_state=0, penalty=\"l2\")\n",
    "fit=svm.fit(X_tr, y_tr)\n",
    "y_pred=svm.predict(X_val)\n",
    "score_4=svm.score(X_val,y_val)\n",
    "conf_mat_4=confusion_matrix(y_val,y_pred)\n",
    "clas_rep_4=classification_report(y_val,y_pred)\n",
    "print(score_4)\n",
    "print(conf_mat_4)\n",
    "print(clas_rep_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in your part 5 conclusions here\n",
    "Since first results were also good, they did not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusion\n",
    "\n",
    "Conclude with a full report here on what we know now about this problem. How well it does verses baseline, what the best Keras archtecture is, what features should be used, how the data should be cleaned etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
